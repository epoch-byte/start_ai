# [一、梯度概念(Gradient)]()
  ## 1.2 梯度
  ### 1.2.1 梯度概念
$
    对于函数z=f(x,y)，在平面区域D内具有一阶连续偏导数，则对于每一点(x_{0}, y_{0})∈D,都可以定义出一个向量：f_{x}(x_{0},y_{0})*\vec i + f_{y}(x_{0},y_{0})*\vec j,该向量称为函数f(x,y)在(x_{0},y_{0})的梯度，记作grad f(x_{0},y_{0})或者 \bigtriangledown f(x_{0},y_{0}),其中 \bigtriangledown = \frac{\partial}{\partial x}*\vec i + \frac{\partial}{\partial y}*\vec j​称为向量微分算子或者Nabla算子
$。
  ### 1.2.2 梯度上升/下降
  发现梯度就定义为偏导向量的方向。而方向导数一节已经证明，沿着偏导向量方向的方向导数 $D_{u}f(x,y)$能够取得最大值。因此在不断的迭代计算中，每一次沿着负梯度方向进行更新参数，就能够达到最低点
# [二、机器学习为什么要用梯度上升、梯度下降算法]()
## 2.1 问题求解过程

  - 分析问题，确定问题分类

  - 确定假设函数

  - 定义损失(或者代价)函数

  - 求解目标函数权重系数
    * 对于损失函数，采用梯度下降算法，快速达到最低点；对于最大值问题，采用梯度上升算法，快速达到最高点
    * 采用最小二乘法计算

## 2.2 通过梯度下降（迭代更新算法）—对回归参数的迭代


## 2.3 通过最小二乘法 —求解权重系数

# [三、梯度下降常用算法]()

## 批量梯度下降算法
在经典的随机梯度下降算法(批量梯度下降)中，迭代下降公式：$$x_{t+1} = x_{t}+\alpha \bigtriangledown f(t)$$
以一元线性回归为例：
假设函数：$$f(x) = wx+b$$
损失函数：$$Loss(x) = \sum_{i=1}^{N}(wx_{i}+b - y_{i})^2$$
其梯度表达为:
$$
\begin{align} 
\left(\frac{\partial Loss}{\partial w}, \frac{\partial Loss}{\partial b}\right) & = \left(\sum_{i=1}^{n}(2x_{i}(wx_{i}+b-y_{i})), \sum_{i=1}^{n}(2*(wx_{i}+b-y_{i}))\right)  
     \\ & = \left(2*\sum_{i=1}^{n}(x_{i}(wx_{i}+b-y_{i})), 2*\sum_{i=1}^{n}(wx_{i}+b-y_{i})\right)
\end{align}
$$ 
可以看到，这里的梯度计算，使用了所有的样本数据。倘若数据集有 1000 组数据，那就需要计算 1000 次才可以得到梯度，倘若数据集有一亿组数据，就需要计算一亿次，其时间复杂度是O(n) 。当样本数据较多时，对于模型的求解，学习一次的过程是很浪费时间的。

举例：使用只含有一个特征的线性回归来展开。
线性回归的假设函数为：
  $$
   H_{\theta}(x^{(i)}) = \theta _{1} x^{(i)} + \theta _{0} 
  $$
其中 i = 1,2,3...n，其中n表示样本数，$\theta _{j}$为权重系数，j为权重系数个数。
对应的目标函数（代价函数）即为：
$$
  \begin{align}
   Loss(\theta,b) & = \frac{1}{2n}\sum_{i=1}^{n}(H_{\theta}(x^{(i)})-y^{(i)})^2 
   \\ & = \frac{1}{2n}\sum_{i=1}^{n}(\theta _{1} x^{(i)} + \theta _{0} - y^{(i)})^2
  \end{align}
$$
$\colorbox{yellow}{\displaystyle 批量梯度下降法是指在每一次迭代时使用所有样本来进行梯度的更新}$

梯度回归步骤如下：
 - 对目标函数的$\theta _{j}$变量求导
   $$
    \frac{\nabla Loss(\theta _{1},\theta _{0})}{\nabla \theta _{j}} = \frac{1}{n} \sum_{i=1}^{n}(H_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}
   $$
 其中 i = 1,2,3...n，n表示样本数，j表示权重系数个数，这里使用了偏置项$x_{0}^{(i)}=1$

 - 迭代回归训练数据，更新每个权重系数的梯度
    $$
      \begin{align}
      \theta _{j} & := \theta _{j} - \alpha*\frac{\nabla Loss(\theta _{1},\theta _{0})}{\nabla \theta _{j}} 
      \\ & := \theta _{j} - \alpha* \frac{1}{n} \sum_{i=1}^{n}(H_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}
      \end{align}
    $$
$\colorbox{yellow}{\displaystyle注意：这里更新时存在一个求和函数，即为对所有样本进行计算处理，可与下文SGD法进行比较}$

优点：
- 一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。
- 由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。

缺点：
- 当样本数目n很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。
- 从迭代的次数上来看，BGD迭代的次数相对较少

## 随机梯度下降算法(BGD)
为解决批量梯度下降法学习一次浪费时间的问题，因此，可以在所有的样本数据中选择随机的一个实例，用这个实例所包含的数据计算“梯度”。此时的梯度为
$$
\begin{align} 
 \left(  \frac{\partial Loss}{\partial w}, \frac{\partial Loss}{\partial b} \right) = \left(2x_{i}*(w x_{i}+b-y_{i}), 2*(wx_{i}+b-y_{i})\right)
\end{align}
$$ 
其中$(x_{i},y_{i})$是一个随机选中的样本。
到了这里，可能会存在一定的疑问，因为目标函数（代价函数）
$$Loss(x) = \sum_{i=1}^{N}(wx_{i}+b - y_{i})^2$$
其梯度并不是
$$
\begin{align} 
\left(\frac{\partial Loss}{\partial w}, \frac{\partial Loss}{\partial b}\right) = \left(2x_{i}(wx_{i}+b-y_{i}), 2*(wx_{i}+b-y_{i})\right)
\end{align}
$$ 
$\colorbox{Orange}{那么这个方向还是不是可以使目标函数值下降的方向？只能说，对于一次迭代而言，不一定，但是站在宏观的角度去考虑，最后还是很有机会收敛到近似最优解的。}$
事实上，目标函数可以写成:
$$
  H(x) = \sum_{i=1}^{n}S(i) = \sum_{i=1}^{n} (wx_{i}+b - y_{i})^2 
$$
所以梯度是：$$\sum_{i=1}^{n} \nabla S(i)$$
$\colorbox{Orange}{
这时，优化目标是所有样本的损失函数之和，所以在梯度下降时，自然而然是朝着使总的偏差缩小的方向去移动的。而对于随机梯度下降，每一步迭代的优化目标函数不是始终不变的，其变化的范围就是
}$
$$S(i),i=1,2,3...$$
在第 i 步，随机地选中S(i) 作为优化目标，其梯度便是:
$$
\begin{align} 
\left(\frac{\partial Loss}{\partial w}, \frac{\partial Loss}{\partial b}\right) = \left(2x_{i}(wx_{i}+b-y_{i}), 2*(wx_{i}+b-y_{i})\right)
\end{align}
$$ 
而在第i+1步，我们的优化目标可能就变成了: $min S(j)$,此时，梯度也自然变成了
$$
\left(\frac{\partial Loss}{\partial w}, \frac{\partial Loss}{\partial b}\right) = \left(2x_{j}(wx_{j}+b-y_{j}), 2*(wx_{j}+b-y_{j})\right)
$$ 

$\color{Orange}{
显然，随机梯度下降迭代过程中，考虑的下降方向并不是全局下降方向，而是使得某个随机选中的样本的损失函数下降的方向。
在一步迭代中，这种局部样本的下降未必会导致全局损失的下降，但是当迭代次数足够的时候，绝大部分样本都会被考虑到，最终一步一步走向全局最优解。}$
$\color{Orange}{
所以，随机梯度下降相对于梯度下降而言，其根本区别在于每一步迭代时需要优化的目标函数不同。对于经典的梯度下降，
其每一步的目标函数（损失函数）是一样的，即所有样本的（平均）损失函数之和。而对于随机梯度下降而言，其每一步的目标函数是被随机选中的某个样本的损失函数，并不是一直不变的。
}$

[点击观看，SGD演示视频](https://www.zhihu.com/zvideo/1308443683624992768)
上面的每个小球，可以将其理解为随机梯度下降过程中由于随机性而带来的迭代情况的分支。正是由于这种随机性的存在，每个球可以较为自由地选择运动方向，有些就停在某个位置，有些则一路向下。
当迭代的次数足够多时，总会有某个球的路径十分顺畅，最终到达全局最优解的附近。随机梯度下降相对于经典梯度下降，其逃离局部最优的能力更强。因为一旦到达了某个样本的局部最优，随着目标函数的更换，很可能不再是另一个样本的局部最优，迭代就可以继续进行。

当然，$\color{Orange}{随机梯度下降的缺点也是存在的，即它很可能无法收敛到全局最优解。}$ 
什么是全局最优，是 $\sum_{i=1}^{n}(ax_{i}+b-y_{i})^2$ 达到最小嘛？还是每一个S(i) 都无法继续下降？
一般而言，前者可能更容易衡量一些，我们 $\color{Orange}{也更偏向于使用总体的最优作为全局最优，而非每一个样本的最优。}$ 而对于随机梯度下降，即使已经达到了总体全局最优，对于某些样本而言，其可能依然可以继续下降，所以一旦选中了这些样本，就要偏离全局最优点。所以随机梯度下降最终的收敛性确实值得考虑。

但总的来说，随机梯度下降还是很不错的，特别是对于大样本的处理情况，每一次迭代中 O(1) 的计算开销无疑会轻松很多，至于最终的收敛问题，则要根据迭代次数，终止准则等进行一个衡量取舍啦。

### 随机梯度下降求解过程
$\color{yellow}{随机梯度下降法不同于批量梯度下降，随机梯度下降是每次迭代使用一个样本来对参数进行更新。使得训练速度加快。}$
对于一个样本的目标函数为：
$$
J^{(i)}(\theta _{0},\theta _{1}) = \frac{1}{2}(h_{\theta}(x^{(i)})-y^{i})^2
$$
(1)对目标函数求偏导：
$$
\frac {\partial J^{(i)}(\theta _{0},\theta _{1})}{\theta _{j}} =  (h_{\theta}(x^{(j)})-y^{i})x_{j}^{(i)}
$$
(2)迭代回归，更新参数：
$$
\begin{align}
\theta & := \theta - \alpha * \frac {\partial J^{(i)}(\theta _{0},\theta _{1})}{\theta _{j}} 
  \\  & := \theta - \alpha * (h_{\theta}(x^{(j)})-y^{(i)})x_{j}^{(i)}
\end{align}
$$
其中，$\alpha$为学习率

## 小批量梯度下降算法
在了解了经典的梯度下降和随机梯度下降，并且知道其不同之处主要在于迭代过程中目标函数选择的不同。经典梯度下降虽然稳定性比较强，但是大样本情况下迭代速度较慢；随机梯度下降虽然每一步迭代计算较快，但是其稳定性不太好，而且实际使用中，参数的调整往往更加麻烦。
所以，为了协调稳定性和速度，小批量梯度下降应运而生。
小批量梯度下降法和前面两种梯度下降的主要区别就是每一步迭代过程中目标函数的选择不同。
小批量梯度下降是从n个样本中随机且不重复地选择m个进行损失函数的求和
$$
Loss(w,b) = \sum_{i=1}^{m}(wx_{i}+b-y_{i})^2
$$
并将其作为每一步迭代过程中的目标函数。此时，迭代公式中的梯度也就变成了
$$
\begin{align}
\left( \frac{\partial loss(w,b)}{\partial w},\frac{\partial loss(w,b)}{\partial b} \right) = \left( 2\sum_{i=1}^{m}x_{i}(wx_{i}+b-y_{i}), 2\sum_{i=1}^{m}(wx_{i}+b-y_{i}) \right)
\end{align}
$$
显然，m=1时，小批量梯度下降就是随机梯度下降，m=n时，小批量梯度下降就是经典梯度下降。同时，我们也把经典的梯度下降方法称之为全批量梯度下降。
这里的 m为批量大小，其值的选择对于收敛的稳定性和速度有着较大的影响，也是一个技术活。
其他的也没什么好分析的了，基本上和随机梯度下降差不多。

小批量梯度下降，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：每次迭代使用batch_size个样本来对参数进行更新。
这里我们假设 batchSize=10，样本数m=1000

优点：
- 通过矩阵运算，每次在一个 batch 上优化神经网络参数并不会比单个数据慢太多。
- 每次使用一个 batch 可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。
   (比如上例中的30W，设置 batch_size=100 时，需要迭代 3000 次，远小于 SGD 的 30W 次)
- 可实现并行化。

缺点：
  batcha_size的选择带来的影响
- 在合理地范围内，增大batch_size的好处：
  * 内存利用率提高了，大矩阵乘法的并行化效率提高。
  * 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。
  * 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。
- 盲目增大batch_size的坏处：
  * 内存利用率提高了，但是内存容量可能撑不住了。
  * 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。
  * Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。

# [四、梯度上升常用算法]()
$\color{Green}{梯度上升是一种迭代算法，用于寻找函数的局部最大值或全局最大值。\\ 核心思想是: 沿着函数梯度的正方向进行迭代更新，以逐步接近最大值点。在每一次迭代中，根据当前位置的梯度方向来更新参数或变量值，使目标函数值增大。}$
$\color{Orange}{梯度上升算法适用于求解优化问题中的约束最优化、最大似然估计等。}$
梯度上升和梯度下降类似，只不过方向不同，结合下面公式理解。
$$
 \theta := \theta + \alpha*\frac {\partial J^{(i)}(\theta _{0},\theta _{1})}{\theta _{j}}
$$