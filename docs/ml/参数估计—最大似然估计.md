# [极大似然估计方法(Maximum Likelihood Estimate，MLE)]()

# [前言]()
- 1822年首先由德国数学家高斯（C. F. Gauss）在处理正态分布时首次提出；
- 1921年，英国统计学家罗纳德·费希尔（R. A. Fisher）证明其相关性质，得到广泛应用，数学史将其归功于费希尔。
- 研究问题本质背后的深刻原因在于，现实世界本身就是不确定的，人类的观察能力是有局限性的，就是利用已知的样本信息，反向推导最有可能（即最大概率）导致这些样本结果出现的模型参数值。
- 极大似然估计，提供了一种给定观察数据来评估模型参数的方法。也就是似然函数的直观意义是刻画参数 与样本数据的匹配程度。

# [似然与概率区别]()
- 在统计学中，似然函数（likelihood function，通常简写为likelihood，似然）是一个非常重要的内容，在非正式场合似然和概率（Probability）几乎是一对同义词，但是在统计学中似然和概率却是两个不同的概念。
- 概率是在特定环境下某件事情发生的可能性，也就是结果没有产生之前依据环境所对应的参数来预测某件事情发生的可能性；
比如抛硬币，抛之前我们不知道最后是哪一面朝上，但是根据硬币的性质我们可以推测任何一面朝上的可能性均为50%，这个概率只有在抛硬币之前才是有意义的，抛完硬币后的结果便是确定的；
- 似然，刚好相反，是在确定的结果下去推测产生这个结果的可能环境（参数）；
还是抛硬币的例子，假设我们随机抛掷一枚硬币1,000次，结果500次人头朝上，500次数字朝上（实际情况一般不会这么理想，这里只是举个例子），我们很容易判断这是一枚标准的硬币，两面朝上的概率均为50%，这个过程就是我们根据结果来判断这个事情本身的性质（参数），也就是似然。
- 似然的重要前提条件：训练样本的分布能代表样本的真实分布。每个样本集中的样本都是所谓独立同分布的随机变量 (iid条件)，且有充分的训练样本

- 贝叶斯定理采用先验概率和条件概率来解决问题，但是在实际问题中并不都是这样幸运的，我们能获得的数据可能只有有限数目的样本数据，而先验概率和类条件概率(各类的总体分布)都是未知的；
根据仅有的样本数据进行分类时，一种可行的办法是我们需要先对先验概率和类条件概率进行估计，然后再套用贝叶斯分类器。

# [极大似然求解过程]()
### [一般求解步骤]()
求极大似然函数估计值的一般步骤：
- 写出似然函数；
- 对似然函数取对数，并整理；
- 似然函数求导数 ；
- 解似然方程 

以下极大似然估计法的具体做法：
根据总体的分布，建立似然函数$L(x_{1},x_{2},...,x_{n};\theta_{1},\theta_{2},...,{\theta_{n}});$
当L关于$\theta$可微时，(由微积分求极值的原理）可由方程组:
$$
\frac{\partial L}{\partial \theta_{i}} = 0,i=1,2,...,k
$$
定出$\hat{\theta}_{i}(i=1,2,...,k)$，称以上方程组为似然方程；
因为L与$ln$有相似的极大值点，所以$\hat{\theta}_{i}(i=1,2,...,k)$也是有方程组：
$$
\frac{\partial lnL}{\partial \theta_{i}} = 0,i=1,2,...,k
$$
定出$\hat{\theta}_{i}(i=1,2,...,k)$，称以上方程组为对数似然方程；其中$\hat{\theta_{i}}(i=1,2,...,k)就是所求参数\theta_{i}(i=1,2,...,k)$的极大似然估计量。

### [离散型数据求解]()
- $若总体X为离散型，其概率分布列为P(X=x) = p(x;\theta)$,其中\theta为未知参数。
- $设(X_{1},X_{2},...,X_{n})是取自总体X的样本容量为n的样本，则(X_{1},X_{2},...,X_{n})的联合分布律为\prod_{i=1}^{n}p(x_{i},\theta)$.
- $易知样本(X_{1},X_{2},...,X_{n})取到的观测者(x_{1},x_{2},...,x_{n})的概率为L(\theta) = L(x_{1},x_{2},...,x_{n};\theta) = \prod_{i=1}^{n}p(x_{i},\theta)$,这一概率随θ的取值而变化，它是θ的函数，称L(θ)为样本的似然函数。
- $极大似然估计法原理就是固定样本观测值(x_{1},x_{2},...,x_{n}),挑选参数θ使L(x_{1},x_{2},...,x_{n}; \hat{\theta}) = max L(x_{1},x_{2},...,x_{n}; \theta)$
- $得到的\hat{\theta}与样本值有关，\hat{\theta}(x_{1},x_{2},...,x_{n})称为参数θ的极大似然估计值,其相应的统计量\hat{\theta}(X_{1},X_{2},...,X_{n}) 称为θ的极大似然估计量$
- $问题是如何求出参数\theta的极大似然估计\hat{\theta}$。其中利用$lnL(\theta)$是$ln(\hat{\theta})$的增函数，故$lnL(\theta)$和$ln(\hat{theta})$在同一点达到最大值,
于是对似然函数$L(\theta)$取对数，利用微分知识转化为求解对数似然方程
$$
\frac{\partial lnL(\theta)}{\partial \theta_{j}} = 0, j = 1,2,...,k
$$
- 解此方程并对解做进一步的判断。如果最值存在，此方程组求得的驻点即为所求的最值点，就可以很到参数的极大似然估计。
极大似然估计法一般属于这种情况，所以可以直接按上述步骤求极大似然估计。

$\color{Orange}{例1}：有两外形相同的箱子，各装100个球，一箱99个白球1个红球，一箱1个白球99个红球，现从两箱中任取一箱，并从箱中任取一球,结果是白球，问：所取的球来自哪一箱 ?  答：甲箱。$
![!\[\](../image/mle.png)](../image/mle.png)
由于样本集中的样本都是独立同分布，可以只考虑一类样本集 D，来估计参数向量 θ。记已知的样本集为：$D={x_{1},x_{2},...,x_{n}}$
似然函数（linkehood function）：联合概率密度函数$P(D|\theta)$称为相对于${x_{1},x_{2},...,x_{n}}$的θ的似然函数。
$$
l(\theta) = p(D|\theta) = p({x_{1},x_{2},...,x_{n}}|\theta) =\prod_{i=1}^{n} p(x_{i}|\theta))
$$
如果$\hat{\theta}$是参数空间中能使似然函数l(θ)最大的θ值，则$\hat{\theta}$应该是“最可能"的参数值,那么$\hat{\theta}$就是$\theta$d的极大似然估计值。
它是样本集的函数，记作：$\hat{\theta} = d(x_{1},x_{2},...,x_{n})= d(D)$，$\hat{\theta}(x_{1},x_{2},...,x_{n})$称作极大似然函数估计值  
求解极大似然函数,
ML估计：求使得出现该组样本的概率最大的 θ 值。
$$\hat{\theta} = \underset{\theta}{arg \, max}\,l(\theta) = \underset{\theta}{arg \, max}\,\prod_{i=1}^{n}p(x_{i}|\theta) $$
实际中为了便于分析，定义了对数似然函数：
$H(\theta) = ln(l(\theta))$
$\hat{\theta} = \underset{\theta}{arg \, max}\,H(\theta) = \underset{\theta}{arg \, max}\,ln(l(\theta))  = \underset{\theta}{arg \, max}\,\sum_{i=1}^{n} ln(p(x_{i}|\theta)) 
$

### [连续性数据求解]()
- 若总体 X 为连续型，其概率密度函数为 $f(x;\theta)$，其中θ为未知参数。
- $设(X_{1},X_{2},...,X_{n})是取自总体X的样本容量为n的样本，则(X_{1},X_{2},...,X_{n})的联合概率密度函数为\prod_{i=1}^{n}f(x_{i};\theta)$.
- $设(X_{1},X_{2},...,X_{n})$的一组观测值为$(x_{1},x_{2},...,x_{n})$,则随机点(X_{1},X_{2},...,X_{n})落在点$(x_{1},x_{2},...,x_{n})（边长分别为 
dx_{1},dx_{2},...,dx_{n}的n维立方体）$内的概率近似地为$\prod_{i=1}^{n}f(x_{i};\theta)dx_{i}$.
- 函数$L(\theta) = L(x_{1},x_{2},...,x_{n};\theta) = \prod_{i=1}^{n}f(x_{i};\theta)dx_{i}$,
$L(\theta)$ 称为样本的似然函数 
- $得到的\hat{\theta}与样本值有关，\hat{\theta}(x_{1},x_{2},...,x_{n})称为参数θ的极大似然估计值,其相应的统计量\hat{\theta}(X_{1},X_{2},...,X_{n}) 称为θ的极大似然估计量$
- $问题是如何求出参数\theta的极大似然估计\hat{\theta}$。其中利用$lnL(\theta)$是$ln(\hat{\theta})$的增函数，故$lnL(\theta)$和$ln(\hat{theta})$在同一点达到最大值,
于是对似然函数$L(\theta)$取对数，利用微分知识转化为求解对数似然方程
$$
\frac{\partial lnL(\theta)}{\partial \theta_{j}} = 0, j = 1,2,...,k
$$
- 解此方程并对解做进一步的判断。如果最值存在，此方程组求得的驻点即为所求的最值点，就可以很到参数的极大似然估计。
极大似然估计法一般属于这种情况，所以可以直接按上述步骤求极大似然估计。

$\color{Orange}{例}：设X~N(\mu,\sigma^{2});\mu,\sigma^{2}为未知参数，x_{1},x_{2},...,x_{n}是来自X的一个样本值，求\mu,\sigma^{2}的最大似然估计$
$\color{black}{解}$：X的概率密度函数为：
$$
f(x;\mu,\sigma^{2}) = \prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^{2}}(x_{i}-\mu)^{2}}
$$
于是得到似然函数为：
$$
L(\mu,\sigma^{2}) = \prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^{2}}(x_{i}-\mu)^{2}}
$$

对似然函数取对数
$$
lnL = -\frac{n}{2}ln(2\pi) -\frac{n}{2}\sigma^{2} - \frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\mu)^{2}
$$

对参数求偏导，构建似然方程组：
$$
\frac{\partial lnL}{\partial x} = \frac{1}{\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\mu) = 0
\frac{\partial lnL}{\partial \sigma^{2}} = \frac{1}{2((\sigma)^{2})^{2}}\sum_{i=1}^{n}(x_{i}-\mu) - \frac{n}{2(\sigma)^{2}} = 0
$$
求解可得：
$$
\hat{\mu} = \frac{1}{n}\sum_{i=1}^{n}x_{i} = \overset{-}{x}
$$
$$
\hat{(\sigma^{2})} = \frac{1}{n}\sum_{i=1}^{n}(x_{i}-\mu)^{2}  
$$
故$\mu,\sigma^{2}$的极大似然估计量分别为:
$$
\frac{1}{n}\sum_{i=1}^{n}X_{i} = \overset{-}{X}, \frac{1}{n}\sum_{i=1}^{n}(X_{i}-\overset{-}{X}) = S^{2}
$$



----------------------
https://blog.csdn.net/qq_41775769/article/details/113514294