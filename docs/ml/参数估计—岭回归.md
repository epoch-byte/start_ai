# [岭回归-Ridge Regression]()
## [引言]()
最小二乘法(OLS)求解，需假设$X^{T}X$为满秩矩阵，然而现实任务中$X^{T}X$往往不是满秩矩阵或者某些列之间的线性相关性比较大。
例如存在许多任务中，会出现变量数(属性数)远超过样例数，导致X的列数多于行数，$X^{T}X$显然不满秩，即$X^{T}X$的行列式接近于0，即接近于奇异，此时计算是误差会很大，可解出多个解(方程组数少于未知数的个数时，没有唯一解，即有无穷多个解)，它们都能使均方误差最小化。在多元回归中，特征之间会出现多重共线问题，使用最小二乘法估计系数会出现系数不稳定问题，缺乏稳定性和可靠性。


## [岭回归原理]()
岭回归的核心思想是在最小二乘法(OLS)的基础上引入一个正则化项，通过对回归系数进行调整来解决多重共线性问题。正则化项是一个惩罚项，它能够约束回归系数的大小，降低模型的复杂度，防止过拟合。

岭回归的系数估计公式如下:
$$
  \color{Black}{\mathbf{ \hat \beta ^{bridge} = (X^{T}X+ \lambda I)^{-1}X^{T}y}}
$$
其中，$\hat \beta ^{bridge}$ 是岭回归系数的估计值；X 是自变量矩阵；у 是因变量向量；$\lambda$ 是岭参数,控制正则化的程度；I 是单位矩阵。

$\mathbf{当\lambda为0时,岭回归退化为最小二乘法。当\lambda增大时,正则化的作用变得更加明显,岭回归系数的估计值趋向于0}$

## [岭回归特点]()
优点
- 解决多重共线性问题：岭回归能够有效地处理自变量之间存在高度相关性的情况，提高回归系数估计的稳定性。
- 控制过拟合：通过引入正则化项，岭回归可以降低模型的复杂度，减少过拟合的风险。
- 灵活性：岭回归的岭参数可以根据实际情况进行调整，使模型更加灵活适用于不同的数据集和问题。

缺点
- 岭参数的选择：选择合适的岭参数需要一定的经验和技巧，过大或过小的岭参数都可能导致不良的结果。
- 系数解释性：由于岭回归对回归系数进行了调整，因此解释岭回归模型的系数可能相对复杂。

## [现实应用]()
- 与交叉验证等方法结合使用：通常通过交叉验证等方法来选择最佳的岭参数，以及评估模型的性能。
- 在机器学习中的应用：岭回归的思想被推广到其他机器学习算法中，如岭分类和岭主成分分析，以解决不同领域中的相关问题。
---
参考：
https://blog.csdn.net/weixin_44225602/article/details/112912067