# [贝叶斯估计(Bayes)]()
## [前言]()
判别式模型和生成式模型是机器学习中两种不同的建模方法，它们在处理数据、构建模型以及实现目标上有着不同的理念和方法。下面是对这两者的详细解释。

### 判别式模型（Discriminative Model）
#### 定义
判别式模型关注的是直接学习输入数据𝑋与输出标签𝑌之间的映射关系，即直接学习条件概率P(Y∣X)。它通过判别边界将不同类别区分开来，通常用于分类和回归问题。
#### 特点
- 关注条件概率：判别式模型直接建模条件概率P(Y∣X)，忽略了输入数据的生成过程。
- 目标：通过优化损失函数，找到最能区分不同类别的决策边界。
- 优势：通常在分类精度上表现较好，特别是在高维数据和复杂模型中。由于不需要建模数据的生成过程，模型训练往往更高效。

#### 常见的判别式模型
- 逻辑回归（Logistic Regression）：线性分类模型，直接估计P(Y∣X)。
- 支持向量机（SVM）：通过寻找最大化类别间隔的超平面进行分类。
- 神经网络（Neural Networks）：通过多层非线性变换进行复杂的映射和分类。
- 条件随机场（CRF）：用于序列标注问题，通过条件概率建模序列数据。
### 生成式模型（Generative Model）
#### 定义
生成式模型关注的是学习输入数据𝑋和输出标签𝑌的联合概率分布P(X,Y)，即通过建模数据的生成过程来推导出条件概率P(Y∣X)。
#### 特点
- 关注联合概率：生成式模型首先学习联合概率P(X,Y)，然后通过贝叶斯公式$P(Y∣X)=\frac {P(X)}{P(X,Y)}$,计算条件概率。
- 目标：能够生成数据，即不仅能进行分类，还能模拟或生成与观测数据相似的样本。
- 优势：生成式模型通常具有更强的解释性和更好的鲁棒性，因为它们了解数据的生成过程，可以应对缺失数据或噪声数据的情况。
#### 常见的生成式模型
- 朴素贝叶斯（Naive Bayes）：假设特征条件独立，通过贝叶斯定理估计类别。
- 高斯混合模型（GMM）：假设数据由多个高斯分布混合生成，通过期望最大化（EM）算法估计参数。
- 隐马尔可夫模型（HMM）：用于序列数据，通过状态转移和观测生成模型建模。
- 生成对抗网络（GAN）：由生成器和判别器组成，通过对抗训练生成逼真的数据样本。
- 变分自编码器（VAE）：结合生成模型和变分推断，进行数据生成和编码。
### 判别式模型与生成式模型的区别与联系
- 学习目标：
  * 判别式模型：直接学习P(Y∣X)，关注如何有效地分类或回归。
  * 生成式模型：首先学习P(X,Y)，关注数据的生成过程以及从生成过程推导分类或回归。
- 数据要求：
  * 判别式模型：通常需要大量的标注数据，注重分类精度和泛化能力。
  * 生成式模型：能在少量数据上进行有效学习，并能生成新数据，具有更强的解释性。
- 模型应用：
  * 判别式模型：主要用于分类和回归任务，尤其在需要高精度分类的场景下。
  * 生成式模型：除了分类和回归，还能用于数据生成、缺失数据填补、异常检测等任务。
### 总结
判别式模型和生成式模型代表了两种不同的建模思路。判别式模型专注于如何准确分类或预测，而生成式模型则兼顾数据生成和分类的能力。选择哪种模型取决于任务的具体需求，如对分类精度、数据生成能力、模型解释性的侧重不同。
## [朴素贝叶斯原理]()
朴素贝叶斯算法基于贝叶斯定理和特征条件独立假设。
- [贝叶斯定理](../math/贝叶斯定理.md)
- 特征条件独立：特征条件独立假设X的n个特征在类确定的条件下都是条件独立的。大大简化了计算过程，但是因为这个假设太过严格，所以会相应牺牲一定的准确率。这也是为什么称呼为朴素的原因。

## [朴素贝叶斯算法]()
### 算法分析
设输入空间$X⊆R_{n}$为n维向量的集合，输出空间为分类标记集合$Y={c_{1},c_{2},c_{3},...,c_{k}}$,输入特征向量$x \in X$,输出分类标记为$y\in Y$，P(X,Y)是X和Y的联合概率分布，数据集
$$
	T = \lbrace (x_{1},y_{1}),(x_{2},y_{2}),(x_{3},y_{3}),...,(x_{n},y_{n}) \rbrace
$$
由P(X,Y)独立同分布产生。

$\color{Orange}{朴素贝叶斯法就是通过训练集来学习联合概率分布P(X,Y)。具体就是从先验概率分布和条件概率分布入手，俩概率相乘即可得联合概率}$

条件概率分布假设如下：$$P(X=x|Y=c_{k}) = P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{(n)}|Y=c_{k}),其中k=1,2,...,K$$
在条件独立性假设条件下，公式等价于：$$\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_{k})$$
对于给定的输入向量x,通过学习到的模型计算后验概率分布$P(Y=c_{k}∣X=x)$,后验分布中最大的类作为x的输出结果，根据贝叶斯定理可知后验概率为
$$
  P(Y=c_{k}∣X=x) = \frac{P(X=x|Y=c_{k})P(Y=c_{k})}{P(X=x)} = \frac{P(X=x|Y=c_{k})P(Y=c_{k})}{\sum_{k}P(X=x|Y=c_{k})P(Y=c_{k})}
$$
所有$c_{k}$的P ( X = x ) P(X=x)P(X=x) 都是相同的，这样我们可以把输出结果化简成
$$
  y = arg \ \overset{}{\underset{c_{k}}{max}}\ P(Y=c_{k})\prod_{j=1}^{n} P(X=x|Y=c_{k})
$$
### 算法流程如下：
- 首先计算Y的K个先验概率,$P(Y=c_{k})$
- 然后计算条件概率分布：
  $$P(X=x|Y=c_{k}) = P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{(n)}|Y=c_{k})$$
  由于上式的参数是指数级别，无法计算。所以根据特征条件独立假设，可以化简为下式。
  $$P(X=x|Y=c_{k}) = \prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_{k})$$
- 根据贝叶斯原理，计算后验概率
  $$
  P(Y=c_{k}∣X=x) = \frac{P(X=x|Y=c_{k})P(Y=c_{k})}{P(X=x)} = \frac{P(X=x|Y=c_{k})P(Y=c_{k})}{\sum_{k}P(X=x|Y=c_{k})P(Y=c_{k})}
  $$
  由于分母相同，上式简化后为如下：
  $$
  P(Y=c_{k}∣X=x) = \prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_{k})P(Y=c_{k}）
  $$
- 计算$X(test)$的类别
  $$
    y(test) = arg \ \overset{}{\underset{c_{k}}{max}}\ P(Y=c_{k})\prod_{j=1}^{n} P(X^{(j)}=x_{test}^{(j)}|Y=c_{k})
  $$
  从上面的计算可以看出，没有复杂的求导和矩阵运算，因此效率很高。
## [条件概率——极大似然估计]()
从上节知道对于给定的输入向量x，其输出结果可以表示为：
$$
  y = arg \ \overset{}{\underset{c_{k}}{max}}\ P(Y=c_{k})\prod_{j=1}^{n} P(X=x|Y=c_{k})
$$
可以使用极大似然估计法来估计相应的概率。
先求先验概率:$P(Y=c_{k})$
$$P(Y=c_{k}) = \frac{\sum_{i=1}^{n}I(y_{i}=c_{k})}{n},k=1,2,\ldots,K$$

设第j个特征x(j)可能的取值的集合为$\left \{ a_{j1},a_{j2},\ldots,a_{js_j} \right \}$,条件概率$P(X^{j}=a_{jl}|Y=c_{k})$的极大似然估计是：
 $$
 \begin{align}
 P(X^{j}=a_{jl} |Y =c_{k}) = \frac{\sum_{i=1}^{n}I(x_{i}^{(j)},y_{i}=c_{k})}{\sum_{i=1}^{n}I(y_{i}=c_{k})} \\
 \\
 其中，样本序列 j= 1,2,...,n; 特征取值集合 l = 1，2，...,s_{j} ; 输出分类 k=1,2,...,K
 \end{align}
 $$


 
## [朴素贝叶斯法学习与分类算法]()
- 输入: 训练数据$\ T = \lbrace (x_{1},y_{1}),(x_{2},y_{2}),(x_{3},y_{3}),...,(x_{n},y_{n}) \rbrace, 其中 x_{i}= ( x_{i}^{(1)},x_{i}^{(2)}, x_{i}^{(3)},...,x_{i}^{(n)})^{T}, x_{i}^{(j)} \in \left \{ a_{j1},a_{j2},\ldots,a_{js_j} \right \},j = 1,2,...,n,l = 1,2,...,S_{j},y_{i} \in \lbrace c_{1},c_{2},...,c_{K} \rbrace;$
- 输出：实例x的分类
- 算法流程如下
  * 计算先验概率及条件概率
 $$
 \begin{align}
 P(Y=c_{k}) & = \frac{\sum_{i=1}^{N}I(Y=c_{k})}{N} \\
 P(X^{j} = a_{jl} |Y =c_{k}) & = \frac{\sum_{i=1}^{n}I(x_{i}^{(j)},y_{i}=c_{k})}{\sum_{i=1}^{n}I(y_{i}=c_{k})}
 \end{align}
 $$
  * 对于给定的实例$x=(x^{(1)},x^{(2)}, x^{(3)},...,x^{(n)})^{T}$,计算
 $$ P(Y=c_{k}∣X=x) =P(Y=c_{k})\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_{k}),\  k=1,2,...,K$$
 
  * 确定实例x的分类
 $$y = arg \ \overset{}{\underset{c_{k}}{max}}\ P(Y=c_{k})\prod_{j=1}^{n} P(X=x|Y=c_{k})$$
## [朴素贝叶斯算法小结]()

优点
- 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。
- 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练。
- 对缺失数据不太敏感，算法也比较简单，常用于文本分类。

缺点
- 朴素贝叶斯模型的特征条件独立假设在实际应用中往往是不成立的。
- 如果样本数据分布不能很好的代表样本空间分布，那先验概率容易测不准。
- 对输入数据的表达形式很敏感。


---
参考：
https://blog.csdn.net/qq_30611601/article/details/79343928
https://www.cnblogs.com/BlairGrowing/p/14879028.html