# [定义]()
最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小 。
最小二乘法还可用于曲线拟合，其他一些优化问题也可通过最小化能量或最大化熵用最小二乘法来表达。

# [最小二乘法基本思想]()
最小二乘法是由勒让德在19世纪发现的，形式如下式：
$$　　　　
目标函数 = \sum (观测值-理论值)^2 
$$
观测值是多组样本，理论值就是假设拟合函数。目标函数是机器学习中常说的损失函数，目标是得到使目标函数最小化时候的拟合函数的模型。
举一个最简单的线性回归的简单例子，如有m个只有一个特征的样本:$(x_{i},y_{i})$(i=1,2,...,m)样本采用$h_{\theta}(x)$的多项式拟合函数：
$$
h_{\theta}(x) = \theta _{1}x+ \theta _{0} , (\theta _{0},\theta _{1}为参数)
$$

$\color{Orange}{问题转化为——求解对应的拟合函数有两个参数 }$$\theta _{0},\theta _{1}$。
其目标函数为：
$$
\begin{align}
J(\theta _{0},\theta _{1}) & = \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2 & = \sum_{i=1}^{m}(\theta _{1}*x^{(i)}+\theta _{0}-y^{(i)})^2
\end{align}
$$
目的是最小化目标函数
$$
\begin{align}
minJ(\theta _{0},\theta _{1}) = min \sum_{i=1}^{m}(\theta _{1}*x^{(i)} + \theta _{0} - y^{(i)})^2
\end{align}
$$

# [最小二乘法的代数法解法]()
要使 $J(\theta _{0},\theta _{1})$最小，方法就是对$\theta _{0},\theta _{1}$分别求偏导数，令偏导数为0，得到一个关于$\theta _{0},\theta _{1}$的二元方程组。求解这个二元方程组，就可以得到$\theta _{0},\theta _{1}$的值。

$J(\theta _{0},\theta _{1})$对$\theta _{0}$求偏导，得到如下方程：
$$
\begin{align}
\frac{\partial J(\theta _{0},\theta _{1})}{\partial \theta _{0}} = 2*\sum_{i=1}^{m}(\theta _{1}*x^{(i)}+\theta _{0}-y^{(i)}) = 0
\end{align}
$$
$J(\theta _{0},\theta _{1})$对$\theta _{1}$求偏导，得到如下方程：
$$
\begin{align}
\frac{\partial J(\theta _{0},\theta _{1})}{\partial \theta _{1}} = 2*\sum_{i=1}^{m}x^{(i)}(\theta _{1}*x^{(i)}+\theta _{0}-y^{(i)}) = 0
\end{align}
$$
将3和4组成二元一次方程组，求解$\theta _{0},\theta _{1}$
$$
\begin{cases}
 \sum_{i=1}^{m}(\theta _{1}*x^{(i)}+\theta _{0}-y^{(i)}) = 0  \\
 \sum_{i=1}^{m}x^{(i)}(\theta _{1}*x^{(i)}+\theta _{0}-y^{(i)}) = 0
\end{cases}
$$
$$
\begin{align}
\theta _{0} & = \frac{\sum_{i=1}^{m}(x^{(i)})^2\sum_{i=1}^{m}y^{(i)} - \sum_{i=1}^{m}x^{(i)}\sum_{i=1}^{m}x^{(i)}y^{(i)}}{m\sum_{i=1}^{m}(x^{(i)})^2-(\sum_{i=1}^{m}x^{(i)})^2} \\
\theta _{1} & = \frac{m \sum_{i=1}^{m}x^{(i)}y^{(i)} - \sum_{i=1}^{m}x^{(i)} \sum_{i=1}^{m}y^{(i)}}{m \sum_{i=1}^{m}(x^{(i)})^2-(\sum_{i=1}^{m}x^{(i)})^2}
\end{align}
$$

## 推广到多个样本特征的线性拟合
拟合函数表示为$h_{\theta}(x_{1},x_{2},x_{3},...,x_{n}) = \theta _{0} + \theta _{1}x_{1} + ...+\theta _{n}x_{n}$,其中$theta_{i}$(i=1,2,..,n)为模型参数，$x_{i}$(i=1,2,..,n)为每个样本的特征值，为了简化表示，增加一个特征$x_{0}$ =1，这样拟合函数表示为：
$$
h_{\theta}(x_{1},x_{2},x_{3},...,x_{n}) = \sum_{i=1}^{n}\theta _{i}x_{i}
$$
目标函数表示为：
$$
\begin{align}
J(\theta _{0},\theta _{1},...,\theta _{n}) & = \sum_{j=1}^{m}(h_{\theta}(x_{1}^{(j)},x_{2}^{(j)},x_{3}^{(j)},...,x_{n}) - y^{(j)})^2 \\
 & = \sum_{j=1}^{m}(\sum_{i=1}^{n}\theta _{i}x_{i}^{(j)}- y^{(j)})^2
 \end{align}
$$

利用目标函数分别对$theta_{i}$(i=1,2,..,n)求导，并令导数为0
可得：$\sum_{j=1}^{m}(\sum_{i=1}^{n}\theta _{i}x_{i}^{(j)}- y^{(j)})x_{i}^{(j)},(i=1,2,...,n)$
得到一个N+1元一次方程组，这个方程组有N+1个方程，求解这个方程，就可以得到所有的N+1个未知的θ
 
# [最小二乘法的几何解释]()
最小二乘法的几何意义是高维空间中的一个向量在低维子空间的投影
考虑这样一个简单的问题，求解二元一次方程组：
$$
\begin{cases}
 x_{1} + x_{2} = 3 \leftarrow a \\
 -x_{1} + x_{2} = 1 \leftarrow b
\end{cases}
$$
方程组的解也就是直线$a$与$b$的交点，并且很容易算出，$x_{1}$=1,$x_{2}$=2.它的矩形形式:

$$
\begin{bmatrix}
1\\
-1
\end{bmatrix}
*x_{1}+
\begin{bmatrix}
1\\
1
\end{bmatrix}
*x_{2} = b \Leftrightarrow a_{1}*x_{1} + a_{2}*x_{2} = b
$$
表示$x_{1}$倍的向量$a_{1}$加上$x_{2}$的$a_{2}$倍的向量等于向量b。或者说，b是向量$a_{1}$与$a_{2}$的线性组合。
![!\[\](../image/min2.png)](../image/min2.png)
可以看到，1倍的$a_{1}$加上2倍的$x_{2}$既是b，而1和2正是我们的解。而最小二乘所面临的问题远不止两个点，拿三个点来说吧。（0,2）,（1,2）,（2,3）
![!\[\](../image/min1.png)](../image/min1.png)
假设我们要找到一条直线 y=kx+b穿过这三个点（虽然不可能），为表述方便，用$x_{1}$代替k,$x_{2}$代替b:
$$
\begin{cases}
  1 * k + b = 2 \\
  0 * k + b = 2 \\
  2 * k + b = 3 
\end{cases}
\Leftrightarrow
\begin{cases}
  1 * x_{1} + x_{2} = 2 \\
  0 * x_{1} + x_{2} = 2 \\
  2 * x_{1} + x_{2} = 3 
\end{cases}
\Leftrightarrow
\begin{bmatrix}
1 & 1 \\
0 & 1 \\
2 & 1 
\end{bmatrix}
\begin{bmatrix}
 x_{1} \\
 x_{2} 
\end{bmatrix} =
\begin{bmatrix}
2 \\
2 \\
3
\end{bmatrix}
\Leftrightarrow 
A * x = b
$$
进一步的：
$$
\begin{bmatrix}
1\\
0\\
2
\end{bmatrix} *
x_{1} +
\begin{bmatrix}
1\\
1\\
1
\end{bmatrix} * x_{2} 
\Leftrightarrow
a_{1}*x_{1} + a_{2} * x_{2} = b
$$
向量b是向量$a_{1}$与$a_{2}$的线性表示。用图形表示：
![!\[\](../image/min0.png)](../image/min0.png)
作图之后，我们惊讶的发现，无论我们怎样更改$a_{1}$和$a_{2}$的系数都不可能得到b，因为$a_{1}$和$a_{2}$的线性组合成的向量只能落在它们组成的子空间S里面，也就是说，向量不在平面S上，虽然我们找不到这样的向量，但在S上找一个比较接近的可以吧。很自然的想法就是将向量b投影在平面S上，投影在S上的向量P就是b的近似向量，并且方程$A\hat x = P$是有解的。
![!\[\](../image/min4.png)](../image/min4.png)
这个误差最小的时候就是e正交与平面S,也正交与S中的向量$a_{1}$,$a_{2}$(矩阵A的列向量)，即点乘为0，$a_{1}^{T}$e = 0, $a_{2}^{T}e=0$ 矩阵表示：
$A^{T}e = 0$ 
$A^{T}(b-A \hat x) = 0$
$A^{T}Ax =A^{T}b$
 

# [最小二乘法的矩阵法解法]()


# [最小二乘法的局限性和适用场景]()


---
# [参考]()
https://www.cnblogs.com/BlairGrowing/p/14847772.html