# 损失函数

## [一、损失函数](#损失函数)

* [损失函数（Loss Function)]()：是定义在单个样本上的，是指一个样本的误差
* [代价函数（Cost Function)]()：是定义在整个训练集上的，是所有样本误差的平均，也就是所有损失函数值的平均。
* [目标函数（Object Function)]()：是指最终需要优化的函数，一般来说是经验风险+结构风险，也就是（代价函数+正则化项）。

损失函数用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。
损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。

损失函数：一次预测的好坏；
风险函数：平均意义下模型预测的好坏；

## [二、常用损失函数](#常用损失函数)

### [2.1 0-1损失函数]()
      0-1 损失是指预测值和目标值不相等为 1， 否则为 0。
$$
L(Y,f(x)) =
\begin{cases}
1, & \text{Y } \neq \text{ f(x)} \\
0, & \text{Y } = \text{ f(x)}
\end{cases}
$$

      特点：
      0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用
      感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足 |Y-f(x)|<T 时认为相等，
$$
L(Y,f(x)) =
\begin{cases}
1, & |Y-f(x)|<T \\
0, & |Y-f(x)|\ge T
\end{cases}
$$


### [2.2 平方损失函数]()
      平方损失函数标准形式如下：
$$
      L(Y,f(x)) = (Y-f(x))^2
$$
### [2.3 绝对损失函数]()
      绝对值损失函数是计算预测值与目标值的差的绝对值：
$$
      L(Y,f(x)) = |Y-f(x)| 
$$

### [2.4 对数损失函数]()
      log对数损失函数的标准形式如下：

$$
      L(Y,P(Y|X)) = -logP(Y|X) = -\cfrac{1}{N}\sum_{i=1}^N \sum_{j=1}^N y_{ij}log(P_{ij})
$$

　    特点：
      
      log对数损失函数能非常好的表征概率分布，在很多场景尤其是多分类，如果需要知道果属于每个类别的置信度，那它非常适合
      
      健壮性不强，相比于hinge loss对噪声更敏感；
      
      逻辑回归的损失函数就是log对数损失函数。
### [2.5 指数损失函数]()
      指数损失函数形式如下：
$$
      L(Y|f(x)) = exp[-yf(x)]
$$
      特点: 
      
      对离群点、噪声非常敏感。经常用在AdaBoost算法中。——不理解 ？
### [2.6 Hinge 损失函数]()
      Hinge 损失函数标准形式如下：
$$
      L(y,f(x)) = max(0,1-yf(x))
$$
      特点：

      hinge损失函数表示如果被分类正确，损失为0，否则损失就为 1-yf(x) 。SVM 就是使用这个损失函数；
      
      一般的f(x) 是预测值，在 （-1,1）范围内 之间，y 是目标值 （-1或1）。其含义是，f(x) 的值在（-1,1）范围内就可以了，并不鼓励 |f(x)|>1,即并不鼓励分类器过度自信，让某个正确分类的样本距离分割线超过1并不会有任何奖励，从而使分 类器可以更专注于整体的误差；

      健壮性相对较高，对异常点、噪声不敏感，但它没太好的概率解释。
   

### [2.7 感知损失函数]()
      感知损失函数的标准形式如下：
$$
      L(y,f(x)) = max(0,1-yf(x))
$$
　    特点：
      
      感知损失函数是Hinge损失函数的一个变种，Hinge loss对判定边界附近的点(正确端)惩罚力度很高。而perceptron loss只要样本的判定类别正确的话，它就满意，不管其判定边界的距离。它比Hinge loss简单，因为不是max-margin boundary，所以模型的泛化能力没 hinge loss强。
### [2.8 交叉熵损失函数 (Cross-entropy loss function)](交叉熵损失函数)
     
      交叉熵损失函数的标准形式如下:
$$
      C = \cfrac{1}{n}\sum_{x}[ylna+(1-y)ln(1-a)]
$$
      注意: 公式中x 表示样本，y表示实际的标签，a表示预测的输出，n 表示样本总数量。
      特点： 

      1、本质上也是一种对数似然函数，可用于二分类和多分类任务中。
         * 二分类问题中的Loss函数（输入数据是softmax或者sigmoid函数的输出） 
$$
      loss = -\cfrac{1}{n}\sum_{x}[ylna+(1-y)ln(1-a)]
$$
      
         * 多分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）
$$
      loss = -\cfrac{1}{n}\sum_{x}y_{i}lna_{i}
$$

      2、当使用sigmoid作为激活函数的时候，常用交叉熵损失函数而不用均方误差损失函数，因为它可以完美解决平方损失函数权重更新过慢的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。

## [三、常用代价函数](#常用代价函数)
### [3.1 均方误差（Mean Squared Error）]()

$$
      MSE = \cfrac{1}{N}\sum_{i=1}^N(y^{(i)}-f(x^{(i)}))^2
$$
      均方误差是指参数估计值与参数真值之差平方的期望值; MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。（i 表示第i个样本，N 表示样本总数）,通常用来做回归问题的代价函数。

### [3.2 均方根误差]()
$$
      RMSE = \sqrt[2]{\cfrac{1}{N}\sum_{i=1}^N(y^{(i)}-f(x^{(i)}))^2}
$$
      均方根误差是均方误差的算术平方根，能够直观观测预测值与实际值的离散程度。通常用来作为回归算法的性能指标。            

### [3.3 平均绝对误差（Mean Absolute Error）]()

$$
      MAE = \cfrac{1}{N}\sum_{i=1}^N|y^{(i)}-f(x^{(i)})|
$$
      平均绝对误差是绝对误差的平均值 ，平均绝对误差能更好地反映预测值误差的实际情况。通常用来作为回归算法的性能指标。

### [3.4 交叉熵代价函数（Cross Entry）]()

$$
      H(p,q) = -\sum_{i=1}^N p(x^{(i)})logq(x^{(i)})
$$
      交叉熵是用来评估当前训练得到的概率分布与真实分布的差异情况，减少交叉熵损失就是在提高模型的预测准确率。其中 p(x) 是指真实分布的概率，q(x) 是模型通过数据计算出来的概率估计。
      比如对于二分类模型的交叉熵代价函数（可参考逻辑回归一节）：
$$
      L(w,b) = -\cfrac{1}{N}\sum_{i=1}^N(y^{(i)})logf(x^{(i)})+(1-y^{(i)})log(1-logf(x^{(i)}))
$$ 
      其中f(x)可以是sigmoid函数。或深度学习中的其它激活函数。而$$y^{(i)}∈{0,1}$$,通常用做分类问题的代价函数。